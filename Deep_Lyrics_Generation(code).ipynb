{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07Isq5C8Xzj-",
        "outputId": "ecb63bfc-c911-4905-fb0e-e4b9db971e7d"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKXgjybnYMyd"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from string import punctuation\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VSrPLZHaGhC"
      },
      "source": [
        "lyrics = []\n",
        "for year in range(2000, 2005):\n",
        "    df = pd.read_csv('/content/drive/MyDrive/Melon/Melon_{0}.csv'.format(year))\n",
        "    \n",
        "    #가사 데이터를 문장 단위로 나눠 저장\n",
        "    for lyric in list(df.lyric.values):\n",
        "        lyrics.extend(lyric.split('\\n'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_Atsl_pF8a2",
        "outputId": "541d6c56-562b-43ab-fede-c25ae4fa90bd"
      },
      "source": [
        "print(\"문장 개수:\",len(lyrics))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 개수: 173140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWi7SDm1bcMZ"
      },
      "source": [
        "import re\n",
        "\n",
        "not_hangul = re.compile('[^가-힣 +]')\n",
        "def only_hangul(raw_sentence, not_hangul):\n",
        "    # 특수문자 제거\n",
        "    temp = re.sub('[^가-힣a-zA-Z0-9 ]', '', raw_sentence)\n",
        "\n",
        "    # 한글 외의 문자를 포함하지 않는 문장만 전처리\n",
        "    if not_hangul.search(temp) == None:\n",
        "        # 문장 시작, 끝에 위치하는 띄어쓰기 삭제\n",
        "        temp = re.sub('^ | $', '', temp)\n",
        "        return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzAFa7U9e0an",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9575ae0-d30c-400d-8cce-7f3708e20eec"
      },
      "source": [
        "# 전처리 가사 list로 저장\n",
        "only_hangul_lyrics = []\n",
        "for lyric in lyrics:\n",
        "    temp = only_hangul(lyric, not_hangul)\n",
        "    if temp == None or temp == '':\n",
        "        continue\n",
        "    only_hangul_lyrics.append(temp)\n",
        "    \n",
        "print(\"한글만 포함한 문장 개수:\", len(only_hangul_lyrics))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한글만 포함한 문장 개수: 19034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riOA3YsRO6Zx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ceb5a05-8b15-42e3-9914-202ce42b3e73"
      },
      "source": [
        "preprocessed_lyrics = []\n",
        "for lyric in only_hangul_lyrics:\n",
        "    sent1 = lyric\n",
        "    sent2 = ''\n",
        "    \n",
        "    word_list = lyric.split(' ')\n",
        "    if len(word_list) > 10:\n",
        "        continue\n",
        "    elif len(word_list) == 6 or len(word_list) == 7:\n",
        "        sent1 = ' '.join(word_list[:2])\n",
        "        sent2 = ' '.join(word_list[3:])\n",
        "    elif len(word_list) == 8 or len(word_list) == 9:\n",
        "        sent1 = ' '.join(word_list[:3])\n",
        "        sent2 = ' '.join(word_list[4:])\n",
        "    elif len(word_list) == 10:\n",
        "        sent1 = ' '.join(word_list[:4])\n",
        "        sent2 = ' '.join(word_list[5:])\n",
        "    \n",
        "    if sent2 == '':\n",
        "        preprocessed_lyrics.append(sent1)\n",
        "    else:\n",
        "        preprocessed_lyrics.extend([sent1, sent2])\n",
        "\n",
        "print(\"마디 길이 조정한 문장 개수:\", len(preprocessed_lyrics))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "마디 길이 조정한 문장 개수: 21368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "H6P3NDMdLmx-",
        "outputId": "4181b6a9-1eae-4450-d12b-3d6f06929921"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "word_cnts = [len(lyric.split(' ')) for lyric in preprocessed_lyrics]\n",
        "length = [len(i) for i in preprocessed_lyrics]\n",
        "\n",
        "plt.hist(word_cnts, range(1, 10))\n",
        "plt.hist(length, range(1, 30))\n",
        "# 각 가사의 마디를 개행을 기준으로 나눴는데, 입력 형식에 따라 단어의 개수가 너무 다르다\n",
        "# max(len) 에 맞춰 시퀀스를 형성할 경우 불필요한 메모리를 잡아먹으므로, 단어의 개수가 많은 가사는 두 개의 마디로 나눈다\n",
        "# 단어 개수 분포를 봤을 때 3, 4개로 구성된 마디가 가장 많았고 5 개부터 급격히 감소했다.\n",
        "# 단어가 6개인 마디 -> 3개, 3개\n",
        "# 단어가 7개인 마디 -> 3개, 4개\n",
        "# .. 8개 -> 4개, 4개\n",
        "# .. 9개 -> 4개, 5개\n",
        "# .. 10개 -> 5개, 5개\n",
        "# 긴 마디를 두 마디로 분할해 데이터를 형성한다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1.600e+01, 1.450e+02, 6.900e+02, 1.743e+03, 2.572e+03, 2.719e+03,\n",
              "        3.681e+03, 4.699e+03, 5.559e+03, 5.154e+03, 5.001e+03, 4.257e+03,\n",
              "        4.036e+03, 3.371e+03, 2.897e+03, 2.140e+03, 1.237e+03, 6.210e+02,\n",
              "        1.680e+02, 2.000e+01, 3.000e+00, 1.000e+00, 0.000e+00, 0.000e+00,\n",
              "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00]),\n",
              " array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
              "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),\n",
              " <a list of 28 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThmwKl3N6HDR",
        "outputId": "871f2e88-a3f6-4118-ae6b-eb8b30b812cf"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 43.4 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0EU0F8xXdTF"
      },
      "source": [
        "#전처리된 가사에 대해 토큰화 진행\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "\n",
        "tokenized_lyrics = []\n",
        "for sentence in preprocessed_lyrics:\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=False) # 토큰화\n",
        "    tokenized_lyrics.append(tokenized_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P60vJIOejo3c",
        "outputId": "8176bd4b-eceb-4873-881c-273b2943c9d5"
      },
      "source": [
        "# tokenizer로 단어 군집 형성\n",
        "tokenizer = Tokenizer(lower=False)\n",
        "tokenizer.fit_on_texts(tokenized_lyrics)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('단어 집합의 크기: %d'% vocab_size)\n",
        "# 단어 집합의 크기가 매우 크다! 단어에 전처리를 해서 단어 개수를 줄이든, one-hot 벡터 변환 말고 다른 방식으로 학습을 하든 해야한다\n",
        "# softmax와 cross entropy를 사용하는게 아니라 regression 방식으로 접근해서 label을 예측하자"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합의 크기: 1092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slOJJ3MvugXf"
      },
      "source": [
        "tokenized_lyrics = [list(sent) for sent in preprocessed_lyrics]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW5nPtDDpiVW",
        "outputId": "da401102-1a69-4942-fe10-1f3d199fd64e"
      },
      "source": [
        "# tokenizer로 단어 군집 형성\n",
        "tokenizer = Tokenizer(lower=False)\n",
        "tokenizer.fit_on_texts(preprocessed_lyrics)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('단어 집합의 크기: %d'% vocab_size)\n",
        "# 단어 집합의 크기가 매우 크다! 단어에 전처리를 해서 단어 개수를 줄이든, one-hot 벡터 변환 말고 다른 방식으로 학습을 하든 해야한다\n",
        "# softmax와 cross entropy를 사용하는게 아니라 regression 방식으로 접근해서 label을 예측하자"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합의 크기: 16058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Dsxq6DT0d7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a6d0b0d-4d65-4e92-a9ef-cb40e7e41a33"
      },
      "source": [
        "threshold = 2\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합(vocabulary)의 크기 : 16057\n",
            "등장 빈도가 1번 이하인 희귀 단어의 수: 9026\n",
            "단어 집합에서 희귀 단어의 비율: 56.21224388117332\n",
            "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 12.950713824521129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66dU0CEfoj0f",
        "outputId": "50f01efc-4f04-47fb-96c6-02b6766f3618"
      },
      "source": [
        "vocab_size_filter = total_cnt - rare_cnt\n",
        "print('희귀 단어를 제외한 단어 집합의 크기:', vocab_size_filter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "희귀 단어를 제외한 단어 집합의 크기: 7031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpZvR5Ylqo_D"
      },
      "source": [
        "# tokenizer로 정수값을 부여 받은 단어들에 대한 dictionary 생성\n",
        "# index_to_word[3] == '안녕'\n",
        "index_to_word = {}\n",
        "for key, value in tokenizer.word_index.items():\n",
        "    index_to_word[value] = key"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOwRpoX47YFV"
      },
      "source": [
        "# 정수 인코딩된 가사 리스트를 학습에 이용 가능한 시퀀스의 리스트로 변경\n",
        "tokenizer = Tokenizer(vocab_size)\n",
        "tokenizer.fit_on_texts(tokenized_lyrics)\n",
        "\n",
        "sequences = list()\n",
        "\n",
        "for sentence in tokenized_lyrics:\n",
        "    # 문장을 각 단어의 정수값 리스트로 변환한다\n",
        "    # 문장을 거꾸로 학습할 것이므로 단어 순서를 뒤집어 시퀀스를 생성한다\n",
        "    encoded = tokenizer.texts_to_sequences([sentence])[0][::-1]\n",
        "    for i in range(1, len(encoded)):\n",
        "        sequence = encoded[:i+1]\n",
        "        sequences.append(sequence)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj8lUe6PjlCi",
        "outputId": "b1ef755c-2a57-479c-af7c-b1d6cd336890"
      },
      "source": [
        "tokenized_lyrics[1][::-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['곳', '머물다가는', '처럼', '구름']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBE_IBbfp7Gd"
      },
      "source": [
        "# 정수 인코딩된 가사 리스트를 학습에 이용 가능한 시퀀스의 리스트로 변경\n",
        "tokenizer = Tokenizer(vocab_size)\n",
        "tokenizer.fit_on_texts(preprocessed_lyrics)\n",
        "\n",
        "sequences = list()\n",
        "\n",
        "for sentence in preprocessed_lyrics:\n",
        "    # 문장을 각 단어의 정수값 리스트로 변환한다\n",
        "    # 문장을 거꾸로 학습할 것이므로 단어 순서를 뒤집어 시퀀스를 생성한다\n",
        "    encoded = tokenizer.texts_to_sequences([sentence])[0][::-1]\n",
        "    for i in range(1, len(encoded)):\n",
        "        sequence = encoded[:i+1]\n",
        "        sequences.append(sequence)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZnFyhkZXhgL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e86dcb9c-f19b-4418-a163-aa492ceff10c"
      },
      "source": [
        "max_len = max(len(l) for l in sequences)\n",
        "max_len"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hItymZsu7l0i"
      },
      "source": [
        "# 패딩을 이용해 모든 시퀀스의 길이를 통일한다\n",
        "max_len = max(len(l) for l in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "\n",
        "X = padded_sequences[:, :-1]\n",
        "y = padded_sequences[:, -1]\n",
        "y = to_categorical(y, num_classes=vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIsSVfXFTTx0"
      },
      "source": [
        "# 임베딩 모델 학습 및 사전 임베딩 벡터 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLLQB_tYTTVZ"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o3zhVouSQrM"
      },
      "source": [
        "import urllib.request\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from konlpy.tag import Okt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xgvfjQXZl0v"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gMhCt1lZxJQ"
      },
      "source": [
        "w2v = Word2Vec(sentences = tokenized_lyrics, size = 200, window = 5, min_count = 5, workers = 4, sg = 0)\n",
        "w2v.wv.save_word2vec_format('kor_w2v')\n",
        "w2v_model = KeyedVectors.load_word2vec_format('kor_w2v')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIbm3NlhfKBr"
      },
      "source": [
        "w2v_pretrained = Word2Vec.load('/content/drive/MyDrive/ko.bin')\n",
        "w2v_pretrained.wv.save_word2vec_format('kor_w2v_pretrained')\n",
        "pretrained_w2v_model = KeyedVectors.load_word2vec_format('kor_w2v_pretrained')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvKsu099f5xS",
        "outputId": "afbd9a08-2879-4e63-b41d-74051f0f72fd"
      },
      "source": [
        "w2v_model.vectors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5072, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucUyHvZJcaRS"
      },
      "source": [
        "def get_vector(word, word2vec_model):\n",
        "    if word in word2vec_model:\n",
        "        return word2vec_model[word]\n",
        "    else: \n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E-oYtXKfxxX"
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size_filter, 200))\n",
        "for word, index in index_to_word.items():\n",
        "    # 단어와 맵핑되는 사전 훈련된 임베딩 벡터값\n",
        "    vector_value = get_vector(word, w2v_model)\n",
        "    if vector_value is not None:\n",
        "        embedding_matrix[index] = vector_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfgZua7RhGBI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d687300d-f915-4ccd-a639-4531eefca2cc"
      },
      "source": [
        "embedding_dim = 200\n",
        "hidden_units = 32\n",
        "\n",
        "model = Sequential()\n",
        "e = Embedding(vocab_size_filter, embedding_dim, weights=[embedding_matrix], input_length=max_len - 1, trainable=False)\n",
        "model.add(e)\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dense(vocab_size_filter, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, validation_split=0.2, epochs=100, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "4334/4334 - 49s - loss: 6.3918 - accuracy: 0.0293 - val_loss: 6.4271 - val_accuracy: 0.0256 - 49s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "4334/4334 - 43s - loss: 6.3135 - accuracy: 0.0294 - val_loss: 6.4447 - val_accuracy: 0.0256 - 43s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "4334/4334 - 42s - loss: 6.3103 - accuracy: 0.0293 - val_loss: 6.4475 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "4334/4334 - 43s - loss: 6.3075 - accuracy: 0.0296 - val_loss: 6.4505 - val_accuracy: 0.0256 - 43s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "4334/4334 - 43s - loss: 6.3060 - accuracy: 0.0295 - val_loss: 6.4569 - val_accuracy: 0.0256 - 43s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "4334/4334 - 43s - loss: 6.3047 - accuracy: 0.0296 - val_loss: 6.4573 - val_accuracy: 0.0256 - 43s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "4334/4334 - 43s - loss: 6.3035 - accuracy: 0.0295 - val_loss: 6.4579 - val_accuracy: 0.0256 - 43s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "4334/4334 - 43s - loss: 6.3019 - accuracy: 0.0296 - val_loss: 6.4590 - val_accuracy: 0.0256 - 43s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "4334/4334 - 43s - loss: 6.3013 - accuracy: 0.0294 - val_loss: 6.4595 - val_accuracy: 0.0256 - 43s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "4334/4334 - 43s - loss: 6.3005 - accuracy: 0.0297 - val_loss: 6.4598 - val_accuracy: 0.0256 - 43s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "4334/4334 - 42s - loss: 6.2998 - accuracy: 0.0295 - val_loss: 6.4569 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "4334/4334 - 42s - loss: 6.2991 - accuracy: 0.0297 - val_loss: 6.4575 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "4334/4334 - 46s - loss: 6.2987 - accuracy: 0.0297 - val_loss: 6.4559 - val_accuracy: 0.0256 - 46s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "4334/4334 - 45s - loss: 6.2981 - accuracy: 0.0298 - val_loss: 6.4586 - val_accuracy: 0.0256 - 45s/epoch - 10ms/step\n",
            "Epoch 15/100\n",
            "4334/4334 - 42s - loss: 6.2976 - accuracy: 0.0298 - val_loss: 6.4573 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 16/100\n",
            "4334/4334 - 42s - loss: 6.2971 - accuracy: 0.0297 - val_loss: 6.4584 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 17/100\n",
            "4334/4334 - 42s - loss: 6.2966 - accuracy: 0.0298 - val_loss: 6.4614 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 18/100\n",
            "4334/4334 - 42s - loss: 6.2964 - accuracy: 0.0297 - val_loss: 6.4632 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 19/100\n",
            "4334/4334 - 42s - loss: 6.2961 - accuracy: 0.0298 - val_loss: 6.4601 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 20/100\n",
            "4334/4334 - 42s - loss: 6.2957 - accuracy: 0.0298 - val_loss: 6.4620 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 21/100\n",
            "4334/4334 - 42s - loss: 6.2953 - accuracy: 0.0298 - val_loss: 6.4627 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 22/100\n",
            "4334/4334 - 42s - loss: 6.2950 - accuracy: 0.0297 - val_loss: 6.4620 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 23/100\n",
            "4334/4334 - 42s - loss: 6.2948 - accuracy: 0.0298 - val_loss: 6.4642 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 24/100\n",
            "4334/4334 - 42s - loss: 6.2945 - accuracy: 0.0296 - val_loss: 6.4645 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 25/100\n",
            "4334/4334 - 42s - loss: 6.2944 - accuracy: 0.0298 - val_loss: 6.4621 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 26/100\n",
            "4334/4334 - 42s - loss: 6.2941 - accuracy: 0.0298 - val_loss: 6.4636 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 27/100\n",
            "4334/4334 - 42s - loss: 6.2939 - accuracy: 0.0298 - val_loss: 6.4655 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 28/100\n",
            "4334/4334 - 42s - loss: 6.2938 - accuracy: 0.0298 - val_loss: 6.4645 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 29/100\n",
            "4334/4334 - 42s - loss: 6.2936 - accuracy: 0.0298 - val_loss: 6.4665 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 30/100\n",
            "4334/4334 - 42s - loss: 6.2934 - accuracy: 0.0298 - val_loss: 6.4646 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 31/100\n",
            "4334/4334 - 42s - loss: 6.2932 - accuracy: 0.0298 - val_loss: 6.4678 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 32/100\n",
            "4334/4334 - 42s - loss: 6.2932 - accuracy: 0.0298 - val_loss: 6.4659 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 33/100\n",
            "4334/4334 - 42s - loss: 6.2929 - accuracy: 0.0298 - val_loss: 6.4671 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 34/100\n",
            "4334/4334 - 42s - loss: 6.2927 - accuracy: 0.0298 - val_loss: 6.4687 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 35/100\n",
            "4334/4334 - 42s - loss: 6.2926 - accuracy: 0.0298 - val_loss: 6.4659 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 36/100\n",
            "4334/4334 - 42s - loss: 6.2924 - accuracy: 0.0298 - val_loss: 6.4652 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 37/100\n",
            "4334/4334 - 42s - loss: 6.2923 - accuracy: 0.0298 - val_loss: 6.4679 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 38/100\n",
            "4334/4334 - 42s - loss: 6.2921 - accuracy: 0.0298 - val_loss: 6.4667 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 39/100\n",
            "4334/4334 - 42s - loss: 6.2921 - accuracy: 0.0298 - val_loss: 6.4675 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 40/100\n",
            "4334/4334 - 42s - loss: 6.2919 - accuracy: 0.0298 - val_loss: 6.4693 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 41/100\n",
            "4334/4334 - 46s - loss: 6.2918 - accuracy: 0.0298 - val_loss: 6.4698 - val_accuracy: 0.0256 - 46s/epoch - 11ms/step\n",
            "Epoch 42/100\n",
            "4334/4334 - 45s - loss: 6.2918 - accuracy: 0.0298 - val_loss: 6.4676 - val_accuracy: 0.0256 - 45s/epoch - 10ms/step\n",
            "Epoch 43/100\n",
            "4334/4334 - 43s - loss: 6.2916 - accuracy: 0.0298 - val_loss: 6.4674 - val_accuracy: 0.0256 - 43s/epoch - 10ms/step\n",
            "Epoch 44/100\n",
            "4334/4334 - 43s - loss: 6.2916 - accuracy: 0.0298 - val_loss: 6.4695 - val_accuracy: 0.0256 - 43s/epoch - 10ms/step\n",
            "Epoch 45/100\n",
            "4334/4334 - 46s - loss: 6.2914 - accuracy: 0.0298 - val_loss: 6.4698 - val_accuracy: 0.0256 - 46s/epoch - 11ms/step\n",
            "Epoch 46/100\n",
            "4334/4334 - 46s - loss: 6.2913 - accuracy: 0.0298 - val_loss: 6.4703 - val_accuracy: 0.0256 - 46s/epoch - 10ms/step\n",
            "Epoch 47/100\n",
            "4334/4334 - 43s - loss: 6.2913 - accuracy: 0.0298 - val_loss: 6.4695 - val_accuracy: 0.0256 - 43s/epoch - 10ms/step\n",
            "Epoch 48/100\n",
            "4334/4334 - 43s - loss: 6.2912 - accuracy: 0.0298 - val_loss: 6.4687 - val_accuracy: 0.0256 - 43s/epoch - 10ms/step\n",
            "Epoch 49/100\n",
            "4334/4334 - 42s - loss: 6.2911 - accuracy: 0.0298 - val_loss: 6.4700 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 50/100\n",
            "4334/4334 - 42s - loss: 6.2910 - accuracy: 0.0298 - val_loss: 6.4699 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 51/100\n",
            "4334/4334 - 42s - loss: 6.2910 - accuracy: 0.0298 - val_loss: 6.4712 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 52/100\n",
            "4334/4334 - 41s - loss: 6.2908 - accuracy: 0.0298 - val_loss: 6.4711 - val_accuracy: 0.0256 - 41s/epoch - 10ms/step\n",
            "Epoch 53/100\n",
            "4334/4334 - 41s - loss: 6.2908 - accuracy: 0.0298 - val_loss: 6.4721 - val_accuracy: 0.0256 - 41s/epoch - 9ms/step\n",
            "Epoch 54/100\n",
            "4334/4334 - 45s - loss: 6.2907 - accuracy: 0.0298 - val_loss: 6.4716 - val_accuracy: 0.0256 - 45s/epoch - 10ms/step\n",
            "Epoch 55/100\n",
            "4334/4334 - 45s - loss: 6.2907 - accuracy: 0.0298 - val_loss: 6.4714 - val_accuracy: 0.0256 - 45s/epoch - 10ms/step\n",
            "Epoch 56/100\n",
            "4334/4334 - 42s - loss: 6.2906 - accuracy: 0.0298 - val_loss: 6.4719 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 57/100\n",
            "4334/4334 - 41s - loss: 6.2905 - accuracy: 0.0298 - val_loss: 6.4721 - val_accuracy: 0.0256 - 41s/epoch - 10ms/step\n",
            "Epoch 58/100\n",
            "4334/4334 - 40s - loss: 6.2905 - accuracy: 0.0298 - val_loss: 6.4714 - val_accuracy: 0.0256 - 40s/epoch - 9ms/step\n",
            "Epoch 59/100\n",
            "4334/4334 - 40s - loss: 6.2904 - accuracy: 0.0298 - val_loss: 6.4727 - val_accuracy: 0.0256 - 40s/epoch - 9ms/step\n",
            "Epoch 60/100\n",
            "4334/4334 - 40s - loss: 6.2903 - accuracy: 0.0298 - val_loss: 6.4721 - val_accuracy: 0.0256 - 40s/epoch - 9ms/step\n",
            "Epoch 61/100\n",
            "4334/4334 - 40s - loss: 6.2903 - accuracy: 0.0298 - val_loss: 6.4730 - val_accuracy: 0.0256 - 40s/epoch - 9ms/step\n",
            "Epoch 62/100\n",
            "4334/4334 - 40s - loss: 6.2902 - accuracy: 0.0298 - val_loss: 6.4746 - val_accuracy: 0.0256 - 40s/epoch - 9ms/step\n",
            "Epoch 63/100\n",
            "4334/4334 - 40s - loss: 6.2901 - accuracy: 0.0298 - val_loss: 6.4742 - val_accuracy: 0.0256 - 40s/epoch - 9ms/step\n",
            "Epoch 64/100\n",
            "4334/4334 - 40s - loss: 6.2901 - accuracy: 0.0298 - val_loss: 6.4759 - val_accuracy: 0.0256 - 40s/epoch - 9ms/step\n",
            "Epoch 65/100\n",
            "4334/4334 - 41s - loss: 6.2902 - accuracy: 0.0298 - val_loss: 6.4751 - val_accuracy: 0.0256 - 41s/epoch - 9ms/step\n",
            "Epoch 66/100\n",
            "4334/4334 - 40s - loss: 6.2901 - accuracy: 0.0298 - val_loss: 6.4757 - val_accuracy: 0.0256 - 40s/epoch - 9ms/step\n",
            "Epoch 67/100\n",
            "4334/4334 - 40s - loss: 6.2901 - accuracy: 0.0298 - val_loss: 6.4758 - val_accuracy: 0.0256 - 40s/epoch - 9ms/step\n",
            "Epoch 68/100\n",
            "4334/4334 - 40s - loss: 6.2900 - accuracy: 0.0298 - val_loss: 6.4763 - val_accuracy: 0.0256 - 40s/epoch - 9ms/step\n",
            "Epoch 69/100\n",
            "4334/4334 - 44s - loss: 6.2900 - accuracy: 0.0298 - val_loss: 6.4756 - val_accuracy: 0.0256 - 44s/epoch - 10ms/step\n",
            "Epoch 70/100\n",
            "4334/4334 - 45s - loss: 6.2899 - accuracy: 0.0298 - val_loss: 6.4764 - val_accuracy: 0.0256 - 45s/epoch - 10ms/step\n",
            "Epoch 71/100\n",
            "4334/4334 - 42s - loss: 6.2899 - accuracy: 0.0298 - val_loss: 6.4768 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 72/100\n",
            "4334/4334 - 46s - loss: 6.2898 - accuracy: 0.0298 - val_loss: 6.4776 - val_accuracy: 0.0256 - 46s/epoch - 11ms/step\n",
            "Epoch 73/100\n",
            "4334/4334 - 45s - loss: 6.2898 - accuracy: 0.0298 - val_loss: 6.4779 - val_accuracy: 0.0256 - 45s/epoch - 10ms/step\n",
            "Epoch 74/100\n",
            "4334/4334 - 42s - loss: 6.2897 - accuracy: 0.0298 - val_loss: 6.4774 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 75/100\n",
            "4334/4334 - 42s - loss: 6.2897 - accuracy: 0.0298 - val_loss: 6.4789 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 76/100\n",
            "4334/4334 - 42s - loss: 6.2897 - accuracy: 0.0298 - val_loss: 6.4790 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 77/100\n",
            "4334/4334 - 42s - loss: 6.2896 - accuracy: 0.0298 - val_loss: 6.4790 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 78/100\n",
            "4334/4334 - 41s - loss: 6.2896 - accuracy: 0.0298 - val_loss: 6.4791 - val_accuracy: 0.0256 - 41s/epoch - 10ms/step\n",
            "Epoch 79/100\n",
            "4334/4334 - 41s - loss: 6.2895 - accuracy: 0.0298 - val_loss: 6.4790 - val_accuracy: 0.0256 - 41s/epoch - 10ms/step\n",
            "Epoch 80/100\n",
            "4334/4334 - 41s - loss: 6.2895 - accuracy: 0.0298 - val_loss: 6.4792 - val_accuracy: 0.0256 - 41s/epoch - 10ms/step\n",
            "Epoch 81/100\n",
            "4334/4334 - 41s - loss: 6.2895 - accuracy: 0.0298 - val_loss: 6.4800 - val_accuracy: 0.0256 - 41s/epoch - 10ms/step\n",
            "Epoch 82/100\n",
            "4334/4334 - 45s - loss: 6.2895 - accuracy: 0.0298 - val_loss: 6.4800 - val_accuracy: 0.0256 - 45s/epoch - 10ms/step\n",
            "Epoch 83/100\n",
            "4334/4334 - 45s - loss: 6.2894 - accuracy: 0.0298 - val_loss: 6.4806 - val_accuracy: 0.0256 - 45s/epoch - 10ms/step\n",
            "Epoch 84/100\n",
            "4334/4334 - 42s - loss: 6.2894 - accuracy: 0.0298 - val_loss: 6.4797 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 85/100\n",
            "4334/4334 - 42s - loss: 6.2893 - accuracy: 0.0298 - val_loss: 6.4799 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 86/100\n",
            "4334/4334 - 42s - loss: 6.3215 - accuracy: 0.0298 - val_loss: 6.4802 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 87/100\n",
            "4334/4334 - 42s - loss: 6.2893 - accuracy: 0.0298 - val_loss: 6.4808 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 88/100\n",
            "4334/4334 - 42s - loss: 6.3188 - accuracy: 0.0298 - val_loss: 6.4811 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 89/100\n",
            "4334/4334 - 42s - loss: 6.2894 - accuracy: 0.0298 - val_loss: 6.4820 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 90/100\n",
            "4334/4334 - 42s - loss: 6.2894 - accuracy: 0.0298 - val_loss: 6.4824 - val_accuracy: 0.0256 - 42s/epoch - 10ms/step\n",
            "Epoch 91/100\n",
            "4334/4334 - 41s - loss: 6.2893 - accuracy: 0.0298 - val_loss: 6.4823 - val_accuracy: 0.0256 - 41s/epoch - 9ms/step\n",
            "Epoch 92/100\n",
            "4334/4334 - 41s - loss: 6.2894 - accuracy: 0.0298 - val_loss: 6.4822 - val_accuracy: 0.0256 - 41s/epoch - 9ms/step\n",
            "Epoch 93/100\n",
            "4334/4334 - 41s - loss: 6.2894 - accuracy: 0.0298 - val_loss: 6.4832 - val_accuracy: 0.0256 - 41s/epoch - 9ms/step\n",
            "Epoch 94/100\n",
            "4334/4334 - 45s - loss: 6.2894 - accuracy: 0.0298 - val_loss: 6.4834 - val_accuracy: 0.0256 - 45s/epoch - 10ms/step\n",
            "Epoch 95/100\n",
            "4334/4334 - 44s - loss: 6.2894 - accuracy: 0.0298 - val_loss: 6.4841 - val_accuracy: 0.0256 - 44s/epoch - 10ms/step\n",
            "Epoch 96/100\n",
            "4334/4334 - 40s - loss: 6.2895 - accuracy: 0.0298 - val_loss: 6.4846 - val_accuracy: 0.0256 - 40s/epoch - 9ms/step\n",
            "Epoch 97/100\n",
            "4334/4334 - 40s - loss: 6.3198 - accuracy: 0.0298 - val_loss: 6.4834 - val_accuracy: 0.0256 - 40s/epoch - 9ms/step\n",
            "Epoch 98/100\n",
            "4334/4334 - 40s - loss: 6.2895 - accuracy: 0.0298 - val_loss: 6.4836 - val_accuracy: 0.0256 - 40s/epoch - 9ms/step\n",
            "Epoch 99/100\n",
            "4334/4334 - 40s - loss: 6.2895 - accuracy: 0.0298 - val_loss: 6.4838 - val_accuracy: 0.0256 - 40s/epoch - 9ms/step\n",
            "Epoch 100/100\n",
            "4334/4334 - 40s - loss: 6.2896 - accuracy: 0.0298 - val_loss: 6.4837 - val_accuracy: 0.0256 - 40s/epoch - 9ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6c22066390>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK1tYJ3OaqLd",
        "outputId": "ab670c35-9ada-4ba7-8da7-c30fb7b9fc32"
      },
      "source": [
        "!pip install glove_python_binary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting glove_python_binary\n",
            "  Downloading glove_python_binary-0.2.0-cp37-cp37m-manylinux1_x86_64.whl (948 kB)\n",
            "\u001b[K     |████████████████████████████████| 948 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.4.1)\n",
            "Installing collected packages: glove-python-binary\n",
            "Successfully installed glove-python-binary-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-qC5sTBbRoo",
        "outputId": "6107b470-3ca4-4190-ec23-2c95c889da8b"
      },
      "source": [
        "preprocessed_lyrics[:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['창가에 서면', '눈물처럼 떠오르는', '그대의 흰 손']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65gDIw74az43"
      },
      "source": [
        "from glove import Corpus, Glove\n",
        "\n",
        "corpus = Corpus() \n",
        "\n",
        "# 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성\n",
        "corpus.fit(tokenized_data, window=5)\n",
        "glove = Glove(no_components=100, learning_rate=0.05)\n",
        "\n",
        "# 학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20.\n",
        "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbACRFJ6bgeA",
        "outputId": "e0df5df8-e150-4884-a1c0-b592e9042d10"
      },
      "source": [
        "print(glove.most_similar(\"그대\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('보나', 0.3049750351275811), ('고호란', 0.2737284563174261), ('되기', 0.2709175572083837), ('햇볕이', 0.2689548162476483)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyrRa7M3AXBx"
      },
      "source": [
        "# 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve448a0Arjay"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAIBrkrkrr_i"
      },
      "source": [
        "embedding_dim = 100\n",
        "hidden_units = 32\n",
        "drop_rate = 0.2\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dropout(drop_rate))\n",
        "model.add(Dense(vocab_size, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6ociRN8csqd"
      },
      "source": [
        "LR = 0.001\n",
        "\n",
        "opt = Adam(learning_rate=LR)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rODzlQUDczJJ",
        "outputId": "b0c49867-d016-4956-954e-8ca5475312b5"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100\n",
        "\n",
        "history = model.fit(X, y, validation_split=0.2, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "5187/5187 - 51s - loss: 4.2304 - accuracy: 0.2607 - val_loss: 3.8928 - val_accuracy: 0.3005 - 51s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "5187/5187 - 48s - loss: 3.6510 - accuracy: 0.3157 - val_loss: 3.5901 - val_accuracy: 0.3305 - 48s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "5187/5187 - 48s - loss: 3.4205 - accuracy: 0.3380 - val_loss: 3.4483 - val_accuracy: 0.3443 - 48s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "5187/5187 - 48s - loss: 3.2896 - accuracy: 0.3516 - val_loss: 3.3621 - val_accuracy: 0.3595 - 48s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "5187/5187 - 52s - loss: 3.1999 - accuracy: 0.3629 - val_loss: 3.3079 - val_accuracy: 0.3645 - 52s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "5187/5187 - 48s - loss: 3.1332 - accuracy: 0.3703 - val_loss: 3.2605 - val_accuracy: 0.3710 - 48s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "5187/5187 - 48s - loss: 3.0769 - accuracy: 0.3781 - val_loss: 3.2241 - val_accuracy: 0.3790 - 48s/epoch - 9ms/step\n",
            "Epoch 8/100\n",
            "5187/5187 - 52s - loss: 3.0273 - accuracy: 0.3828 - val_loss: 3.2022 - val_accuracy: 0.3826 - 52s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "5187/5187 - 48s - loss: 2.9877 - accuracy: 0.3889 - val_loss: 3.1845 - val_accuracy: 0.3867 - 48s/epoch - 9ms/step\n",
            "Epoch 10/100\n",
            "5187/5187 - 48s - loss: 2.9543 - accuracy: 0.3939 - val_loss: 3.1637 - val_accuracy: 0.3868 - 48s/epoch - 9ms/step\n",
            "Epoch 11/100\n",
            "5187/5187 - 53s - loss: 2.9244 - accuracy: 0.3970 - val_loss: 3.1542 - val_accuracy: 0.3906 - 53s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "5187/5187 - 52s - loss: 2.9000 - accuracy: 0.3999 - val_loss: 3.1459 - val_accuracy: 0.3922 - 52s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "5187/5187 - 52s - loss: 2.8800 - accuracy: 0.4020 - val_loss: 3.1331 - val_accuracy: 0.3912 - 52s/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "5187/5187 - 52s - loss: 2.8567 - accuracy: 0.4042 - val_loss: 3.1291 - val_accuracy: 0.3933 - 52s/epoch - 10ms/step\n",
            "Epoch 15/100\n",
            "5187/5187 - 49s - loss: 2.8410 - accuracy: 0.4061 - val_loss: 3.1293 - val_accuracy: 0.3955 - 49s/epoch - 9ms/step\n",
            "Epoch 16/100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WXZKK_Z5x-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c664e62-3c1f-41a6-d043-8fcb1aaf11b8"
      },
      "source": [
        "model.save('/content/drive/MyDrive/model/model_2005_2010')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/model/model_2005_2010/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/model/model_2005_2010/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f52dad07710> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZzfXPnYl0G_"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upewysnD5jkK"
      },
      "source": [
        "# 팝송 단어와 발음이 유사한 한글 단어로 가사 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AX2Y5e7El6wB"
      },
      "source": [
        "from tensorflow import keras \n",
        "model = keras.models.load_model('/content/drive/MyDrive/model/model_2000_2005')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-b0DMyo3smuW"
      },
      "source": [
        "# 팝송 마디 끝 단어와 유사한 로마자 단어 불러오기\n",
        "\n",
        "current_romans = []\n",
        "for year in range(2010, 2011):\n",
        "    with open('/content/drive/MyDrive/sample_words/{0}_rhyme_pairs.txt'.format(year), 'r') as f:\n",
        "        current_romans.append([word[:-1].lower() for word in f.readlines()])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ5ppCApfXME",
        "outputId": "c732f70e-8118-494f-dda3-0e38394c9930"
      },
      "source": [
        "len(current_romans)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0g202_BxVIU"
      },
      "source": [
        "# (romanize, korean) dictionary 불러오기\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# load data\n",
        "with open('/content/drive/MyDrive/end_words.pickle', 'rb') as fr:\n",
        "    end_words_loaded = pickle.load(fr)\n",
        "\n",
        "roman_kor_pairs = [list(roman_kor_pair.values()) for roman_kor_pair in end_words_loaded]\n",
        "roman_kor_pairs = np.array(roman_kor_pairs).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKZUM8Yjxvx0"
      },
      "source": [
        "# 로마자 -> 한글\n",
        "\n",
        "current_words = []\n",
        "for song in current_romans:\n",
        "    for word in song:\n",
        "        try:\n",
        "            current_words.append(roman_kor_pairs[1][list(roman_kor_pairs[0]).index(word)])\n",
        "        except:\n",
        "            current_words.append('pass')\n",
        "#list(roman_kor_pairs[0]).index(current_words[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMu6IlToDwDQ"
      },
      "source": [
        "# 입력할 단어에 대해 토큰화 진행\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "\n",
        "tokenized_current_words = []\n",
        "for word_list in current_words:\n",
        "    tokenized_words = [okt.morphs(word, stem=False)[::-1] for word in word_list] # 토큰화\n",
        "    tokenized_current_words.append(tokenized_words)\n",
        "tokenized_current_words = [' '.join(word_list) for word_set in tokenized_current_words for word_list in word_set]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLo8a_3DsM3S"
      },
      "source": [
        "def sentence_generation(model, tokenizer, current_word, n):\n",
        "    init_word = current_word\n",
        "    sentence = ''\n",
        "\n",
        "    for _ in range(n):\n",
        "        encoded = tokenizer.texts_to_sequences([current_word])[0]\n",
        "        encoded = pad_sequences([encoded], maxlen=max_len - 1, padding='pre')\n",
        "\n",
        "        result = model.predict(encoded, verbose=0)\n",
        "        result = np.argmax(result, axis=1)\n",
        "\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == result:\n",
        "                break\n",
        "        \n",
        "        current_word = current_word + ' ' + word\n",
        "        sentence = sentence + ' ' + word\n",
        "\n",
        "    sentence = init_word + sentence\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhef1RXd1wQA"
      },
      "source": [
        "max_len = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jfw35mLzfKUf",
        "outputId": "a8270cd6-5c61-45e2-c1a9-98f2965e5ae9"
      },
      "source": [
        "' '.join(sentence_generation(model, tokenizer, '실어', 4).split(' ')[::-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'수 없는 너의 그 실어'"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyEs8ubrzQZE"
      },
      "source": [
        "generated_lyrics = []\n",
        "for word in current_words:\n",
        "    if word == 'pass':\n",
        "        generated_lyrics.append('pass')\n",
        "        continue\n",
        "    generated = ' '.join(sentence_generation(model, tokenizer, word, 4).split(' ')[::-1])\n",
        "    generated_lyrics.append(generated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxXJfT1Ghddi",
        "outputId": "42bc2786-cec9-483e-ce42-ad6bcbbcaa39"
      },
      "source": [
        "generated_lyrics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['수 없는 너의 그 놓나',\n",
              " '수 없는 너의 그 지펴줘',\n",
              " '수 그걸 그걸 지켜보는 너',\n",
              " 'pass',\n",
              " '사랑할 자격 갖춘 나 되어',\n",
              " '난 모두 가져가 내게 주었던',\n",
              " '그땐 우리 함께한 날들 생각나죠',\n",
              " '바랄게 다음번에 너 하나만 사랑할꺼야',\n",
              " '상관없어 하기엔 수단방법 가리지도 마',\n",
              " '아마 넌 그대는 나를 잊지마',\n",
              " '전활 이 감옥에서 나갈 때',\n",
              " '수 없는 너의 그 작은방',\n",
              " '너를 잡던 자꾸만 후회가 돼',\n",
              " '이유로 이렇게 날 외면하려 하나요',\n",
              " '어깨에 아픔만 믿어도 참고 그날을',\n",
              " '함께 알 수 주먹을 쥐고',\n",
              " '난 내겐 벅찬 행복 가득한데',\n",
              " '왜 내가 있으면 난 자꾸',\n",
              " '우리가 하지마 근사한 노랠 불러',\n",
              " '수 없는 너의 그 춰',\n",
              " '내게 안고싶은 어서 너를 만날',\n",
              " '니 안에서 나의 모든 게',\n",
              " '우리 맘 이맘 라라라라라라 라라라라라',\n",
              " '수 없는 너의 그 하지말어',\n",
              " '수 없는 너의 그 랄랄라라라라라라',\n",
              " '사랑할 자격 갖춘 나 되어',\n",
              " '수 없는 너의 그 널렸어',\n",
              " '수 없는 너의 그 춰',\n",
              " '내게 안고싶은 어서 너를 만날',\n",
              " '수 없는 너의 그 세월속에서',\n",
              " '우리 맘 이맘 라라라라라라 라라라라라',\n",
              " '수 없는 너의 그 하지말어',\n",
              " '상냥하고 너의 집앞에서 멀지 않은',\n",
              " '수 없는 너의 그 느껴버렸어',\n",
              " '마치 니 옷을 고르기가 어려워',\n",
              " '수 없는 너의 그 언젠지',\n",
              " '너를 이해하는 사람은 나 뿐야',\n",
              " '어떤 말이라도 내게 먼저 말해줘',\n",
              " '말해야 하는데 네 앞에 서면',\n",
              " '이젠 널 위해 남겨진 사랑하나',\n",
              " '상관없어 하기엔 수단방법 가리지도 마',\n",
              " '수 없는 너의 그 짧았지만',\n",
              " '수 없는 너의 그 있었고',\n",
              " '수 없는 너의 그 만났을땐',\n",
              " '수 없는 너의 그 안되죠',\n",
              " '수 없는 너의 그 카사블랑카',\n",
              " '일요일 두 손 흔들어 주었지',\n",
              " '이미 난 이미 얼어버릴 듯',\n",
              " '할 수 있다면 행복하게 살아줘',\n",
              " '당신과 나와 우리 둘이 함께',\n",
              " '못하고 초침의 새야 새야 이리로',\n",
              " '다른 내 머리속에 모든 것이',\n",
              " '다른 내 머리속에 모든 것이',\n",
              " '수 없는 너의 그 축내는',\n",
              " '나를 숨쉬게 하는 내 인생은',\n",
              " '난 난 아직까지 그대만을 원해',\n",
              " '이젠 널 위해 남겨진 사랑하나',\n",
              " '수 없는 너의 그 세월속에서',\n",
              " '우리 맘 이맘 라라라라라라 라라라라라',\n",
              " '나 아직도 그대를 생각하면 아파',\n",
              " '수 없는 너의 그 랄랄라라라라라라',\n",
              " '수 없는 너의 그 섰더라면',\n",
              " '다시 다시 시작한 널 알면서',\n",
              " '수 없는 너의 그 춰',\n",
              " '내게 안고싶은 어서 너를 만날',\n",
              " '수 없는 너의 그 세월속에서',\n",
              " '우리 맘 이맘 라라라라라라 라라라라라',\n",
              " '수 없는 너의 그 하지말어',\n",
              " '상냥하고 너의 집앞에서 멀지 않은',\n",
              " '수 없는 너의 그 느껴버렸어',\n",
              " '수 없는 너의 그 느껴버렸어',\n",
              " '내게 안고싶은 어서 너를 만날',\n",
              " '다른 내 머리속에 모든 것이',\n",
              " '수 없는 너의 그 할걸',\n",
              " '수 없는 너의 그 돌담길',\n",
              " '더 많이 해야만 하는 거니',\n",
              " '내게 안고싶은 어서 너를 만날',\n",
              " '수 없는 너의 그 계절에는',\n",
              " '수 없는 너의 그 버렸네',\n",
              " '매일 또 시작하게 하게 그대처럼',\n",
              " '내게 안고싶은 어서 너를 만날',\n",
              " '다른 내 머리속에 모든 것이',\n",
              " '수 없는 너의 그 할걸',\n",
              " '우리 또 시작하게 하게 버릴때',\n",
              " '수 없는 너의 그 거북이',\n",
              " '내게 안고싶은 어서 너를 만날',\n",
              " '내게 안고싶은 어서 너를 만날',\n",
              " '내게 안고싶은 어서 너를 만날',\n",
              " '상냥하고 너의 집앞에서 멀지 않은',\n",
              " '전활 이 감옥에서 나갈 때',\n",
              " '수 없는 너의 그 춰',\n",
              " '내게 안고싶은 어서 너를 만날',\n",
              " '수 없는 너의 그 세월속에서',\n",
              " '우리 맘 이맘 라라라라라라 라라라라라',\n",
              " '수 없는 너의 그 하지말어',\n",
              " '상냥하고 너의 집앞에서 멀지 않은',\n",
              " '수 없는 너의 그 경험할거야',\n",
              " '수 없는 너의 그 느껴버렸어',\n",
              " '수 없는 너의 그 춰',\n",
              " '이젠 널 위해 남겨진 사랑하나',\n",
              " '수 없는 너의 그 세월속에서',\n",
              " '우리 맘 이맘 라라라라라라 라라라라라',\n",
              " '수 없는 너의 그 하지말어',\n",
              " '상냥하고 너의 집앞에서 멀지 않은',\n",
              " '다시 다시 시작한 널 알면서',\n",
              " '다시 다시 시작한 널 알면서']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXdV_lUrgkC6"
      },
      "source": [
        "with open('/content/drive/MyDrive/generated/generated_2000_2005.txt', 'w') as f:\n",
        "    for sent in generated_lyrics:\n",
        "        f.write(\"%s\\n\" % sent)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}